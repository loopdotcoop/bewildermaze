<!DOCTYPE html>
<html>
<head>
  <meta http-equiv="X-UA-Compatible" content="chrome=1" />
  <meta http-equiv='Content-Type' content='text/html;charset=utf-8'/>
  <title>Canvas Path Test</title>

  <link rel='stylesheet' type='text/css' href='x3dom.css'>
  <link type="text/css" href="../lib/jquery-ui-1.11.3/jquery-ui.css" rel="stylesheet">
</head>
<body style="background-color:#111; color: #999;">

  <audio src="bewhalyer-bubble1.mp3" id="audioA1"></audio>
  <audio src="bewhalyer-bubble2.mp3" id="audioB1"></audio>

  <x3d id="aScene" showStat="false" showLog="false" x="0px" y="0px" width="500px" height="640px">
    <scene>
      <background skyColor='0 0 0'></background>
      <NavigationInfo type="examine"></NavigationInfo>
      <Viewpoint position="2.09873 -1.08607 .5" orientation="0.75664 0.47185 0.45261 1.83860" description=""></Viewpoint>
      <shape>
        <appearance>
          <texture hideChildren="false" id="textureA">
            <canvas width='256' height='256' id='canvasA' style="position:absolute; top:20px;left:520px;">
          </texture>
          <material diffuseColor='0.6 0.9 0.5'></material>  
        </appearance>
        <cone></cone>
      </shape>
      <shape>
        <appearance>
          <texture hideChildren="false" id="textureB">
            <canvas width='256' height='256' id='canvasB' style="position:absolute; top:300px; left:520px;">
          </texture>
          <material diffuseColor='0.6 0.9 0.5'></material>  
        </appearance>
        <sphere></sphere>
      </shape>
    </scene>
  </x3d>
    
  <!--<script src="../lib/x3dom.js"></script>-->
  <script src="../lib/jquery-1.11.2.min.js"></script>
  <script src="../lib/jquery-ui-1.11.3/jquery-ui.min.js"></script>

  <script>

  // https://developer.mozilla.org/en-US/docs/Web/API/MediaStreamAudioSourceNode
  // https://developer.mozilla.org/en-US/docs/Web/API/AnalyserNode

  !function () {

    // Fork getUserMedia for multiple browser versions
    navigator.getUserMedia = (navigator.getUserMedia ||
                              navigator.webkitGetUserMedia ||
                              navigator.mozGetUserMedia ||
                              navigator.msGetUserMedia);


    // Declare variables with app scope
    var audioCtx;
    var splitter;
    var analyserA;
    var analyserB;
    var bufferLength;
    var dataArrayA;
    var dataArrayB;

    var $audioA1;
    var sourceA1;
    var $audioB1;
    var sourceB1;

    var $textureA;
    var $canvasA;
    var ctxA;
    var widthA;
    var heightA;

    var $textureB;
    var $canvasB;
    var ctxB;
    var widthB;
    var heightB;


    // Draw an oscilloscope of the audio input
    var drawA = function () {

      analyserA.getByteTimeDomainData(dataArrayA);

      ctxA.fillStyle = 'rgb(200, 200, 200)';
      ctxA.fillRect(0, 0, widthA, heightA);

      ctxA.lineWidth = 2;
      ctxA.strokeStyle = 'rgb(0, 0, 0)';

      ctxA.beginPath();

      var sliceWidth = widthA * 1.0 / bufferLength;
      var x = 0;
      var doTrigger = false;

      for(var i=0; i<bufferLength; i++) {

        var v = dataArrayA[i] / 128.0;
        var y = v * heightA / 2;

        if (i === 0) {
          ctxA.moveTo(x, y);
        } else {
          ctxA.lineTo(x, y);
        }

        x += sliceWidth;

        if (1.5 < v) { doTrigger = true; }
      }

      ctxA.lineTo(widthA, heightA / 2);
      ctxA.stroke();

      // FIXME: Not the final interface
      $textureA[0]._x3domNode.invalidateGLObject();

      if (doTrigger && $audioA1[0].paused) { $audioA1[0].play(); console.log('play A'); }
      if (doTrigger && ! $audioA1[0].paused) { console.log('already playing A!'); }

      requestAnimationFrame(drawA);
    };


    // Draw an oscilloscope of the audio input
    var drawB = function () {

      analyserB.getByteTimeDomainData(dataArrayB);

      ctxB.fillStyle = 'rgb(200, 200, 200)';
      ctxB.fillRect(0, 0, widthB, heightB);

      ctxB.lineWidth = 2;
      ctxB.strokeStyle = 'rgb(0, 0, 0)';

      ctxB.beginPath();

      var sliceWidth = widthB * 1.0 / bufferLength;
      var x = 0;
      var doTrigger = false;

      for(var i=0; i<bufferLength; i++) {

        var v = dataArrayB[i] / 128.0;
        var y = v * heightB / 2;

        if (i === 0) {
          ctxB.moveTo(x, y);
        } else {
          ctxB.lineTo(x, y);
        }

        x += sliceWidth;

        if (1.5 < v) { doTrigger = true; }
      }

      ctxB.lineTo(widthB, heightB / 2);
      ctxB.stroke();

      // FIXME: Not the final interface
      $textureB[0]._x3domNode.invalidateGLObject();

      if (doTrigger && $audioB1[0].paused) { $audioB1[0].play(); console.log('play B'); }
      if (doTrigger && ! $audioB1[0].paused) { console.log('already playing B!'); }

      requestAnimationFrame(drawB);
    };


    $(function () { // DOM ready

      // Init the canvases
      $textureA = $('#textureA');
      $canvasA  = $('#canvasA');
      widthA  = $canvasA.width();
      heightA = $canvasA.height();

      ctxA      = $canvasA[0].getContext('2d');
      ctxA.fillStyle = "rgb(200,0,0)";  
      ctxA.fillRect(0, 0, 256, 256);

      $textureB = $('#textureB');
      $canvasB  = $('#canvasB');
      widthB  = $canvasB.width();
      heightB = $canvasB.height();

      ctxB      = $canvasB[0].getContext('2d');
      ctxB.fillStyle = "rgb(0,200,0)";  
      ctxB.fillRect(0, 0, 256, 256);

      // Init the audio context
      audioCtx = new (window.AudioContext || window.webkitAudioContext)();

      // Init audio samples
      $audioA1 = $('#audioA1');
      $audioB1 = $('#audioB1');

      // Get access to the audio-input
      if (! navigator.getUserMedia) {
        throw new Error('getUserMedia not supported on your browser!');
      }
      navigator.getUserMedia (

        // Constraints
        { audio: true, video: false },

        // Success callback
        function(stream) {

          // Feed the HTMLMediaElement into a MediaStreamAudioSourceNode
          // http://stackoverflow.com/a/23486702
          window.streamSource = audioCtx.createMediaStreamSource(stream);
          // window.streamSource.channelCountMode      = 'explicit'; //@todo remove?
          // window.streamSource.channelInterpretation = 'discrete'; //@todo remove?

          // Set up the audio analysers
          analyserA = audioCtx.createAnalyser(1);
          analyserA.fftSize = 2048;
          bufferLength = analyserA.frequencyBinCount;
          dataArrayA = new Uint8Array(bufferLength);
          analyserA.getByteTimeDomainData(dataArrayA);

          analyserB = audioCtx.createAnalyser(1);
          analyserB.fftSize = 2048;
          bufferLength = analyserB.frequencyBinCount;
          dataArrayB = new Uint8Array(bufferLength);
          analyserB.getByteTimeDomainData(dataArrayB);

          // Split the streaming audio input into left and right channels
          splitter = audioCtx.createChannelSplitter(2);
          window.streamSource.connect(splitter, 0, 0);

          // Connect the left and right streaming audio inputs to the analysers
          splitter.connect(analyserA, 0);
          splitter.connect(analyserB, 1);

          // Begin the canvas-draw loops
          drawA();
          drawB();
        },

        // Error callback
        function(err) {
          console.log('The following gUM error occured: ' + err);
        }
      );


    });

  }();
  </script>
</body>

</html>
